{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "sns.set()\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#                                                       Outline\n",
    "\n",
    "## Section 1: Decision Trees\n",
    "#### Section 1.1 Intuition Underlying Tree Based Models\n",
    "#### Section 1.2 Applying Tree Based Models\n",
    "#### Section 1.3 Shortcomings & Limitations\n",
    "\n",
    "## Section 2: Ensemble Methods & Random Forests\n",
    "#### Section 2.1 Intuition underlying ensemble approaches\n",
    "#### Section 2.2 Toy Classification Example\n",
    "#### Section 2.3 Time series Regression Example\n",
    "#### Section 2.4 MNIST Dataset Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1.1 Intuition Underlying Tree Based Models\n",
    "### I Spy With My Little Eye...\n",
    "There is a popular game played by children, during road trips, called \"I Spy With My Little Eye\". It involves one person choosing something they saw and the other has to infer said something based on a series of yes/no questions.\n",
    "![alt text](Fig1.pdf \"I Spy With My Little Eye\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During this game, the player partitions the space of total possible objects, with each question defining a new split, till there's just one member left in the accessible space.\n",
    "\n",
    "For instance, the first question takes away the inanimate objects in the space.\n",
    "![alt text](Fig2b.pdf \"I Spy With My Little Eye\")\n",
    "\n",
    "The second question removes the 4-legged friends from the picture.\n",
    "![alt text](Fig2c.pdf \"I Spy With My Little Eye\")\n",
    "\n",
    "The final question removes the terrestrial creatures, in favor of our feathered friends.\n",
    "![alt text](Fig2d.pdf \"I Spy With My Little Eye\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In mathematical parlance, the little girl is solving a multi-class classification problem using disjoint partitions on the space of measurable events using directed acyclic graphs. Used thus, these DAGs are Classification And Regression Trees.\n",
    "![alt text](Fig3a.pdf \"I Spy With My Little Eye\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification & Regression Trees (CARTs) are non-parametric models, that learn such \"splits\" on the feature space from data, to define a mapping from different partitions of the feature space to values on the target space.\n",
    "![alt text](Fig4.pdf \"I Spy With My Little Eye\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1.2 Applying Tree Based Models\n",
    "In this section, we apply tree based models to datasets and analyze their performance to visualize their strangths and weaknesses. But first, we start off by generating some data for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_blobs(n_samples=300, centers=4,random_state=0, cluster_std=0.75)\n",
    "plt.figure(figsize=(7,7))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='gist_rainbow')\n",
    "plt.xlabel(\"$X_1$\")\n",
    "plt.ylabel(\"$X_2$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = DecisionTreeClassifier()\n",
    "tree.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last code cell, first we instantiate the decision tree model (\"tree = DecisionTreeClassifier(max_depth=3)\"). \n",
    "The second step is fitting the model basde on the data. The process of fitting involves evaluating between a series of splits of the feature space and choosing the best at each step via a greedy approach.\n",
    "\n",
    "A tree defines a partition on the space, and every node of the tree defines a split of the dataset. The tree is a sequence of splits, chosen based on the data. At every step, possible splits are evaluated based on increase in homogeneity of data due to the split.\n",
    "\n",
    "![alt text](Fig5.pdf \"I Spy With My Little Eye\")\n",
    "\n",
    "There are various arguments that can be passed to the classifier during instantiation. The primary ones include:\n",
    "\n",
    "a) max_depth=n,  This determines the number of splits in the tree. \n",
    "\n",
    "b) criterion=\"gini\" or \"entropy\", This determines the criterion used to evaluate the split. \n",
    "etc.\n",
    "\n",
    "\n",
    "We shall experiment over the maximum depth criterion, later. For the moment, let's keep that fixed at 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4,4))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=30, cmap='gist_rainbow',clim=(y.min(), y.max()), zorder=3)\n",
    "xx, yy = np.meshgrid(np.linspace(np.min(X[:,0]),np.max(X[:,0]), num=200),np.linspace(np.min(X[:,1]),np.max(X[:,1]), num=200))\n",
    "Z = tree.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "n_classes = len(np.unique(y))\n",
    "contours = plt.contourf(xx, yy, Z, alpha=0.3,\n",
    "                           levels=np.arange(n_classes + 1) - 0.5,\n",
    "                           cmap='gist_rainbow', zorder=1)\n",
    "plt.savefig('Fig6f.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, we vary the maximum depth of the tree from 3 to 6 to 10. As we see in the ensuing figures, the tree's predictions improve a tad, but then quickly devolve to overfitting.\n",
    "![alt text](Fig6a3.pdf \"I Spy With My Little Eye\")\n",
    "![alt text](Fig6c5.pdf \"I Spy With My Little Eye\")\n",
    "![alt text](Fig6e10.pdf \"I Spy With My Little Eye\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Over-fitting turns out to be a property of decision trees. It is very easy to go too deep in the tree, and thus to fit details of the particular data rather than the overall properties of the distributions they are drawn from. \n",
    "![alt text](Fig7.pdf \"I Spy With My Little Eye\")\n",
    "\n",
    "\n",
    "# Section 2. Random Forests\n",
    "In this scenario, we use Ensemble Methods and use a randomized set of such \"trees\" to create a \"forest\". The basis of ensemble methods lies in the famous Condorcet’s Jury Theorem: A group wants to arrive at the “correct” decision via majority vote, wherein each individual has a probability p of voting for the correct decision. What should the size of the group be for optimal performance?\n",
    "It has been shown that if the individual is doing better than random guessing, then the more independent adjudicators in the group, the better the chances of getting the \"right\" answer.\n",
    "\n",
    "Intuitive example: The performance of the “Ask The Audience” lifeline in Who Wants To Be A Millionaire? \n",
    "\n",
    "(92% accuracy for “Ask The Audience” vs only 65% for “Phone A Friend”)\n",
    "\n",
    "In this manner, Aggregating randomized models decreases the variance of the ensemble, while still retaining the low bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2.1 Random Forests: Toy Classification Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(n_estimators=500, random_state=0, max_depth=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,6))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=30, cmap='gist_rainbow',clim=(y.min(), y.max()), zorder=3)\n",
    "xx, yy = np.meshgrid(np.linspace(np.min(X[:,0]),np.max(X[:,0]), num=200),np.linspace(np.min(X[:,1]),np.max(X[:,1]), num=200))\n",
    "Z = model.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "n_classes = len(np.unique(y))\n",
    "contours = plt.contourf(xx, yy, Z, alpha=0.3,\n",
    "                           levels=np.arange(n_classes + 1) - 0.5,\n",
    "                           cmap='gist_rainbow', zorder=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise:\n",
    "In this exercise, we will be applying random forest classifier model on a cancer dataset. The set consists of measurements of 569 patients, with features describing the tumors, and the target classifying the tumors as malignant or benign. \n",
    "\n",
    "We import this dataset, split it into training and testing sets, and train a random forest classifier on this set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 1: Split the data into train and test sets (80%-20%)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Instantiate a random forest classifier model with 100 estimators and a random state of 42. \n",
    "# If you complete this exercise early, Feel free to test the max_depth=4/5/6... hyperparameter too.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Fit the model on the training data, \n",
    "#and evaluate its score on the training and the test data (model.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deriving Insight from the Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "breast_cancer_features = [x for i,x in enumerate(data.feature_names)]\n",
    " \n",
    "def breast_cancer_feature_importances_plot(model):\n",
    "    plt.figure(figsize=(15,8))\n",
    "    n_features = 30\n",
    "    plt.barh(range(n_features), clf.feature_importances_, align='center')\n",
    "    plt.yticks(np.arange(n_features), breast_cancer_features)\n",
    "    plt.title('Random Forest Classifier: Feature Importances')\n",
    "    plt.xlabel(\"Feature Importance\")\n",
    "    plt.ylabel(\"Feature Name\")\n",
    "    plt.ylim(-1, n_features)\n",
    " \n",
    "breast_cancer_feature_importances_plot(clf)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2.2 Random Forests: Time series Regression Example\n",
    "In the previous section we considered random forests within the context of classification. Random forests can also be made to work in the case of regression (that is, continuous rather than categorical variables). The estimator to use for this is the RandomForestRegressor, and the syntax is very similar to what we saw earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(42)\n",
    "x = 10 * rng.rand(200)\n",
    "\n",
    "def modelts(x, sigma=0.3):\n",
    "    fast_oscillation = np.sin(5 * x)\n",
    "    slow_oscillation = np.sin(0.5 * x)\n",
    "    noise = sigma * rng.randn(len(x))\n",
    "    return slow_oscillation + fast_oscillation + noise\n",
    "\n",
    "y = modelts(x)\n",
    "plt.scatter(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest = RandomForestRegressor(200)\n",
    "forest.fit(x[:, None], y)\n",
    "\n",
    "xfit = np.linspace(0, 10, 1000)\n",
    "yfit = forest.predict(xfit[:, None])\n",
    "ytrue = modelts(xfit, sigma=0)\n",
    "\n",
    "plt.scatter(x, y,alpha=0.5)\n",
    "plt.plot(xfit, yfit, '-r');\n",
    "plt.plot(xfit, ytrue, '-k', alpha=0.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The true model is shown in the smooth gray curve, while the random forest model is shown by the jagged red curve. As you can see, the non-parametric random forest model is flexible enough to fit the multi-period data, without us needing to specifying a multi-period model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2.3 Random Forests: MNIST Dataset Classification\n",
    "As a final example, we apply the Random Forest Algorithm to the (too) well known MNIST dataset, for classifying images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = load_digits()\n",
    "fig = plt.figure(figsize=(3, 3)) \n",
    "fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05)\n",
    "\n",
    "for i in range(25):\n",
    "    ax = fig.add_subplot(5, 5, i + 1, xticks=[], yticks=[])\n",
    "    ax.imshow(digits.images[i], cmap=plt.cm.binary, interpolation='nearest')\n",
    "    ax.text(0, 4, str(digits.target[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain, Xtest, ytrain, ytest = train_test_split(digits.data, digits.target,random_state=0)\n",
    "model = RandomForestClassifier(n_estimators=1000)\n",
    "model.fit(Xtrain, ytrain)\n",
    "ypred = model.predict(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "print(metrics.classification_report(ypred, ytest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "mat = confusion_matrix(ytest, ypred)\n",
    "sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False)\n",
    "plt.xlabel('true label')\n",
    "plt.ylabel('predicted label');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y,train_size=0.8 ,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"Accuracy on training data: {:.2f}\".format(clf.score(X_train, y_train)))\n",
    "print(\"Accuracy on test data: {:.2f}\".format(clf.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
